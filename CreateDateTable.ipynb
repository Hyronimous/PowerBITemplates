{"cells":[{"cell_type":"code","execution_count":7,"id":"9ce20ab3-466f-4ea7-b8b2-c2a8a903ef28","metadata":{"collapsed":false},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-03-19T14:37:32.4070709Z","execution_start_time":"2024-03-19T14:37:32.1490448Z","livy_statement_state":"available","parent_msg_id":"3d9463e3-77af-4b2f-8bd8-149cffc8ccf9","queued_time":"2024-03-19T14:37:31.5077668Z","session_id":"a5b9ae49-87bc-448e-b013-0525db9aacc4","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":9},"text/plain":["StatementMeta(, a5b9ae49-87bc-448e-b013-0525db9aacc4, 9, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas, datetime\n","from pyspark.sql import functions as F\n","#initial date range\n","begindate = '2010-01-01'\n","enddate = '2030-12-31'\n","df_panda = pandas.DataFrame({'DateTime':pandas.date_range(start=begindate, end=enddate)})\n","df_panda['Date'] = pandas.to_datetime(df_panda['DateTime']).dt.date\n","\n","df = spark.createDataFrame(df_panda)"]},{"cell_type":"code","execution_count":8,"id":"7e2252ac-eae6-410a-9d14-eb6eb4e015b7","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-03-19T14:37:33.1564862Z","execution_start_time":"2024-03-19T14:37:32.8936502Z","livy_statement_state":"available","parent_msg_id":"0675fd5e-ba67-425f-93f3-11bac093aa72","queued_time":"2024-03-19T14:37:31.5594161Z","session_id":"a5b9ae49-87bc-448e-b013-0525db9aacc4","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":10},"text/plain":["StatementMeta(, a5b9ae49-87bc-448e-b013-0525db9aacc4, 10, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# Get the current date and time to calculate relative columns\n","current_datetime = datetime.datetime.now()\n","\n","# Extract the date component and add to df\n","current_date = current_datetime.date()\n","df = df.withColumn (\"CurrentDate\", F.lit(current_date))"]},{"cell_type":"code","execution_count":9,"id":"dc7cafb0-a9de-4274-885a-0e6ff5730fd6","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-03-19T14:37:33.8655951Z","execution_start_time":"2024-03-19T14:37:33.6195111Z","livy_statement_state":"available","parent_msg_id":"56f2c5a6-7882-48b2-aa43-4e8d2c4047e7","queued_time":"2024-03-19T14:37:31.617771Z","session_id":"a5b9ae49-87bc-448e-b013-0525db9aacc4","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":11},"text/plain":["StatementMeta(, a5b9ae49-87bc-448e-b013-0525db9aacc4, 11, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# add derived columns, weeknumber follows ISO-8601 (week 1 is first week with at least 4 days)\n","df.createOrReplaceTempView(\"DATES\")\n","df_sql = spark.sql(\"\"\"\n","SELECT \n","year(Date) * 10000 + month(Date) * 100 + day(Date) as dateInt, Date, \n","year(date) as year, year(date) - year(CurrentDate) as RelativeYear,\n","quarter(date) as quarter, concat(\"Q\", quarter(date)) as QuarterName, quarter(date) -quarter(CurrentDate) as RelativeQuarter,\n","concat(year(date), \"-\", \"Q\", quarter(date)) as YearQuarter,\n","month(date) as month, month(date) - month(CurrentDate) as RelativeMonth, date_format(Date, 'MMMM') as MonthName, date_format(Date, 'MMM') as MonthNameShort,\n","year(Date) * 100 + month(Date) as MonthYear, concat(date_format(Date, 'MMM'), \"'\", date_format(Date, 'yy')) as MonthYearName,\n","weekofyear(date) as WeekOfYear, concat(\"week \", weekofyear(date)) as WeekOfYearName,\n","case when weekofyear(date) = 53 and month(Date) = 1 then year(date) -1 else year(date) end as ISOYear,\n","case when weekofyear(date) = 53 and month(Date) = 1 then year(date) -1 else year(date) end * 100 + weekofyear(Date) as ISOYearWeek,\n","year(date) * 100 +  weekofyear(Date) as YearWeek,\n","day(date) as day, dayofweek(Date) AS DayOfWeekStartSunday, weekday(Date) + 1 as DayOfWeekStartMonday, \n","date_format(Date, 'EEE') as WeekdayNameShort, date_format(Date, 'EEEE') as WeekdayName,\n","case when weekday(Date) < 5 then \"Y\" else \"N\" end as IsWeekDay,\n","case when weekday(Date) < 5 then \"N\" else \"Y\" end as IsWeekend,\n","case when year(date) = year(CurrentDate) then 'Current' else year(date) end as DefaultYear,\n","case when month(date) = month(CurrentDate) then 'Current' else date_format(Date, 'MMM') end as DefaultMonth,\n","case when quarter(date) = quarter(CurrentDate) then 'Current' else concat(\"Q\", quarter(date)) end as DefaultQuarter\n","FROM DATES\n","\"\"\")\n","#display(df_sql)\n"]},{"cell_type":"code","execution_count":10,"id":"138e29eb-2767-4c40-a005-3f251a2da740","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-03-19T14:37:57.4177815Z","execution_start_time":"2024-03-19T14:37:34.3505804Z","livy_statement_state":"available","parent_msg_id":"854e81fa-2326-4d2e-81b9-59c01b4518b0","queued_time":"2024-03-19T14:37:31.68838Z","session_id":"a5b9ae49-87bc-448e-b013-0525db9aacc4","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":12},"text/plain":["StatementMeta(, a5b9ae49-87bc-448e-b013-0525db9aacc4, 12, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DataFrame has been written to Delta table: CalendarDates\n"]}],"source":["# Delete old Date Table and Create New\n","delta_table_name = \"CalendarDates\"\n","df_sql.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_name)\n","print(f\"DataFrame has been written to Delta table: {delta_table_name}\")"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"e3990cca-e61d-4bfc-847c-280e2661d136","default_lakehouse_name":"DataflowsStagingLakehouse","default_lakehouse_workspace_id":"c6184dd9-bf69-4ec8-8399-a09c1b1c558e"}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
